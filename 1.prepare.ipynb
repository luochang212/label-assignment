{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddf943a-0ca8-4d66-a24e-916f5f26d1c2",
   "metadata": {},
   "source": [
    "# 使用 BLIP-2 生成图片 Embedding\n",
    "\n",
    "为了获取用于聚类的 Embedding，我们用 CIFAR-100 数据集作为图片来源，然后用 BLIP-2 模型生成图片的 Embedding。\n",
    "\n",
    "Huggingface: [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n",
    "\n",
    "> **BLIP-2 (Bootstrapping Language-Image Pre-training 2)** 是 Salesforce 研究院于 2023 年提出的多模态模型。它由三部分组成：\n",
    "> \n",
    "> - 图像编码器（类似 CLIP 的视觉模型）：用于提取图像特征\n",
    "> - 查询变换器（Q-Former）：作为连接图像与文本的桥梁\n",
    "> - 大型语言模型（LLM，如 OPT-2.7B）：用于生成文本 \n",
    "> \n",
    "> BLIP-2 训练时，冻结图像编码器和大语言模型的参数，仅优化 Q-Former，这样既能充分利用已有的单模态能力，又能有效提升图文模型的交互效率。Q-Former 作为连接两个模态的桥梁，通过查询学习将视觉特征转换为与语言模型相兼容的表示，从而实现高效的跨模态对齐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f4d826-0d66-4bcb-8631-b728ca1b9e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:20:24.149048Z",
     "iopub.status.busy": "2025-04-06T07:20:24.148675Z",
     "iopub.status.idle": "2025-04-06T07:20:30.605497Z",
     "shell.execute_reply": "2025-04-06T07:20:30.604019Z",
     "shell.execute_reply.started": "2025-04-06T07:20:24.149022Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import utils\n",
    "\n",
    "from transformers import Blip2Processor, Blip2Model\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Optional\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "CIFAR_PATH = 'data/cifar-100-python/'\n",
    "API_URL = 'http://localhost:8210/embeddings/'\n",
    "SAMPLE_NUM = 10000\n",
    "CSV_PATH = './data'\n",
    "MODEL_PATH = './model/blip2-opt-2.7b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226be547-adb9-45ce-9eb2-a2d9795d78a9",
   "metadata": {},
   "source": [
    "## 1. 下载 CIFAR-100 数据集\n",
    "\n",
    "打开 CIFAR-100 数据集下载页 ( https://www.cs.toronto.edu/~kriz/cifar.html )，点击链接 [CIFAR-100 python version](https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz) 下载数据集。\n",
    "\n",
    "将压缩包 `cifar-100-python.tar.gz` 放在项目的 `./data` 路径下并解压：\n",
    "\n",
    "```\n",
    "cd data\n",
    "tar -xvzf cifar-100-python.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "906dd9fb-2d67-4d4f-ab60-8927101793c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:20:30.607943Z",
     "iopub.status.busy": "2025-04-06T07:20:30.606951Z",
     "iopub.status.idle": "2025-04-06T07:20:30.615658Z",
     "shell.execute_reply": "2025-04-06T07:20:30.614767Z",
     "shell.execute_reply.started": "2025-04-06T07:20:30.607909Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_cifar100_batch(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f, encoding='bytes')\n",
    "\n",
    "    # 解析数据\n",
    "    images = data[b'data']\n",
    "    labels = data[b'fine_labels']  # 细粒度标签（100类）\n",
    "\n",
    "    # 转换图像格式（3072=32x32x3）\n",
    "    images = images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # 若数据格式为 CHW（如 CIFAR-100），转为 HWC\n",
    "    if image.shape[0] in (1, 3):  # 灰度图或 RGB 图\n",
    "        image = image.transpose(1, 2, 0)\n",
    "    # 若数据被归一化到 [0, 1]，还原为 [0, 255]\n",
    "    if np.max(image) <= 1.0:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "    return image\n",
    "\n",
    "def show_image(idx):\n",
    "    if idx < len(train_images):\n",
    "        image = train_images[idx]\n",
    "        image = preprocess_image(image)\n",
    "        display(Image.fromarray(image))\n",
    "    else:\n",
    "        print(f'max idx: {len(train_images) - 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc8626b-8f00-401b-922f-5844d08d4c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:20:30.617063Z",
     "iopub.status.busy": "2025-04-06T07:20:30.616560Z",
     "iopub.status.idle": "2025-04-06T07:20:32.269803Z",
     "shell.execute_reply": "2025-04-06T07:20:32.269234Z",
     "shell.execute_reply.started": "2025-04-06T07:20:30.617036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练集和测试集\n",
    "train_images, train_labels = load_cifar100_batch(os.path.join(CIFAR_PATH, 'train'))\n",
    "test_images, test_labels = load_cifar100_batch(os.path.join(CIFAR_PATH, 'test'))\n",
    "\n",
    "len(train_images), len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37a8fc2-b4c1-46df-b47c-7028f60656d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:20:32.270723Z",
     "iopub.status.busy": "2025-04-06T07:20:32.270551Z",
     "iopub.status.idle": "2025-04-06T07:20:32.281920Z",
     "shell.execute_reply": "2025-04-06T07:20:32.281358Z",
     "shell.execute_reply.started": "2025-04-06T07:20:32.270710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDD8X+Grmbwyt/FdJNc2CLvCqdzITjKn24p1h4X1DT/AISXuvyQsbqWTlGBL+Vux8wPXJ7VueG73UdMTT5bgJ5vlAhSQQ6dASPQjmr93rF7d+GdRbTWuEilSRUQkllYHpk9814ccRGMfZSV9TZw5veRm+AdAb+zdaO2OC98tJBEcZgA5Cn0J64+ldJaatPDoE0MlyIzG28HkMW/yKxtPtYdF0S2s1kzcuglvZe8kp6j8Olcxr+q6pqGuQW+jRlyIj5o27g3PQ+9Y1Ye1rOUXt1GpKELM4nw0uqy6tG+nuxlC8ljldvofavdPDOvjTnbTZYEMsxVxheC7cEivKPBurWOlw3IuHjjUY+bPLAZ6V6JpPxC0pNIe8tbMviI+bGwBbIyM57e1dGLhKpU0VktLk0pJLUseKLOS/NqlluWS5mPmTRuAEUHvnvVDUwPCmnJY2Dot/cSgyXMWNpO7jI7YGRUt74h0jXvCn2y1f7PdeWYrm3PGxgvUH0rxy8uZpb9Iw7YXA4Y8HjnJqqVB29m+hhUV3dH/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 展示第 idx 张图片\n",
    "show_image(idx=1231)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0c4d6-348a-45c7-8a78-c5834f82ab23",
   "metadata": {},
   "source": [
    "## 2. 下载 BLIP-2 模型文件\n",
    "\n",
    "```\n",
    "mkdir model\n",
    "\n",
    "pip install -U huggingface_hub\n",
    "\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "huggingface-cli download --resume-download Salesforce/blip2-opt-2.7b --local-dir ./model/blip2-opt-2.7b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5577d10-84a8-4b7a-883f-6bf0ea4f1d53",
   "metadata": {},
   "source": [
    "## 3. 计算图片 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5594113-8af4-4c9a-9e87-a661c3fe5cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:43:04.152685Z",
     "iopub.status.busy": "2025-04-05T12:43:04.152202Z",
     "iopub.status.idle": "2025-04-05T12:43:04.797994Z",
     "shell.execute_reply": "2025-04-05T12:43:04.797315Z",
     "shell.execute_reply.started": "2025-04-05T12:43:04.152667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b993f10f2e047e0a8c37695a6554027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 模型初始化\n",
    "processor = Blip2Processor.from_pretrained(MODEL_PATH)\n",
    "model = Blip2Model.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba9df06-42e0-4fc0-a910-cbea89f318b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:43:04.798850Z",
     "iopub.status.busy": "2025-04-05T12:43:04.798550Z",
     "iopub.status.idle": "2025-04-05T12:43:05.714912Z",
     "shell.execute_reply": "2025-04-05T12:43:05.714144Z",
     "shell.execute_reply.started": "2025-04-05T12:43:04.798834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1408])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 做简单的推理\n",
    "image = train_images[0]\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.get_image_features(**inputs)\n",
    "    image_embedding = outputs.pooler_output\n",
    "    # image_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ebfe7e8-646c-4247-8c82-14e4ad7bde33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:43:05.715523Z",
     "iopub.status.busy": "2025-04-05T12:43:05.715386Z",
     "iopub.status.idle": "2025-04-05T12:43:05.723423Z",
     "shell.execute_reply": "2025-04-05T12:43:05.722947Z",
     "shell.execute_reply.started": "2025-04-05T12:43:05.715513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0786, -1.5979,  1.6206,  ..., -0.4472,  0.1021, -0.2331]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d985a427-bb14-4295-803d-0d6d7a7818eb",
   "metadata": {},
   "source": [
    "然后进阶一点，把推理步骤写成函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f8e9fc-258e-4f33-b16c-019d8121a60a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:43:05.724570Z",
     "iopub.status.busy": "2025-04-05T12:43:05.724343Z",
     "iopub.status.idle": "2025-04-05T12:43:05.728699Z",
     "shell.execute_reply": "2025-04-05T12:43:05.727921Z",
     "shell.execute_reply.started": "2025-04-05T12:43:05.724551Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path, device):\n",
    "    # 加载 BLIP-2 模型和处理器\n",
    "    processor = Blip2Processor.from_pretrained(MODEL_PATH)\n",
    "    model = Blip2Model.from_pretrained(MODEL_PATH).to(device)\n",
    "\n",
    "    # 加载和预处理图片\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # 获取图片 embedding\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "        image_embedding = outputs.last_hidden_state\n",
    "\n",
    "    return image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66896c3-650d-44e2-9a71-deaa745dd3d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:43:05.729401Z",
     "iopub.status.busy": "2025-04-05T12:43:05.729266Z",
     "iopub.status.idle": "2025-04-05T12:43:07.238431Z",
     "shell.execute_reply": "2025-04-05T12:43:07.237895Z",
     "shell.execute_reply.started": "2025-04-05T12:43:05.729391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8761964f2943d7829659cd8271aa24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 1408])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = \"./img/choice.jpg\"\n",
    "embedding = get_image_embedding(image_path, device=\"cpu\")\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26a00501-ea73-43de-bf4c-27cccc3236db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:43:07.239232Z",
     "iopub.status.busy": "2025-04-05T12:43:07.238909Z",
     "iopub.status.idle": "2025-04-05T12:43:07.243459Z",
     "shell.execute_reply": "2025-04-05T12:43:07.242959Z",
     "shell.execute_reply.started": "2025-04-05T12:43:07.239217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0864, -0.1190, -0.0338,  ..., -0.5104, -0.9064,  0.1509],\n",
       "         [-0.2842,  0.6082, -1.2617,  ...,  0.1604,  0.7182, -1.6816],\n",
       "         [ 0.4250, -0.0169,  0.3701,  ...,  0.5225, -0.7994, -0.6521],\n",
       "         ...,\n",
       "         [ 0.4668,  0.3463, -0.3944,  ...,  0.4220,  0.3917,  0.1684],\n",
       "         [ 0.6901,  0.2861,  0.0472,  ..., -0.0905, -1.6351, -1.6209],\n",
       "         [-0.7165,  0.6343, -0.3295,  ...,  0.0269,  0.9396, -0.7667]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed561dfd-f81d-43f0-a362-191eca8e3ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T11:17:06.301024Z",
     "iopub.status.busy": "2025-04-04T11:17:06.300229Z",
     "iopub.status.idle": "2025-04-04T11:17:06.305666Z",
     "shell.execute_reply": "2025-04-04T11:17:06.304967Z",
     "shell.execute_reply.started": "2025-04-04T11:17:06.300991Z"
    }
   },
   "source": [
    "## 4. 验证 BLIP-2 服务端代码\n",
    "\n",
    "基于 FastAPI 框架，开发 BLIP-2 Embedding 生成服务代码，该服务支持 GPU 批量推理 和 输出向量归一化。服务端代码见本仓库的 [/server/blip2_server.py](./server/blip2_server.py) 文件。\n",
    "\n",
    "运行以下命令，启动 BLIP-2 推理服务：\n",
    "\n",
    "```bash\n",
    "cd server\n",
    "python blip2_server.py\n",
    "```\n",
    "\n",
    "推理服务成功启动后，我们来验证服务端是否如预期运行。\n",
    "\n",
    "以下代码将图片经 base64 编码后传给服务端，服务端接收图片，通过 BLIP-2 模型的正向传播计算，返回图片的 embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c381bc4-32d2-4e4f-b7ff-a9bf326d6f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:43:07.244285Z",
     "iopub.status.busy": "2025-04-05T12:43:07.243981Z",
     "iopub.status.idle": "2025-04-05T12:43:07.247485Z",
     "shell.execute_reply": "2025-04-05T12:43:07.247020Z",
     "shell.execute_reply.started": "2025-04-05T12:43:07.244272Z"
    }
   },
   "outputs": [],
   "source": [
    "API_URL = \"http://localhost:8210/embeddings/\"\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    return encoded_string\n",
    "\n",
    "\n",
    "def client(base64_images):\n",
    "    response = requests.post(\n",
    "        API_URL,\n",
    "        json={\n",
    "            \"base64_images\": base64_images,\n",
    "            \"normalize\": True\n",
    "        },\n",
    "        timeout=30\n",
    "    )\n",
    "    response.raise_for_status()  # 触发 HTTP 错误状态异常\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c5040f0-764a-4963-b697-cd506c253c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T12:43:07.249336Z",
     "iopub.status.busy": "2025-04-05T12:43:07.248949Z",
     "iopub.status.idle": "2025-04-05T12:43:08.940544Z",
     "shell.execute_reply": "2025-04-05T12:43:08.940008Z",
     "shell.execute_reply.started": "2025-04-05T12:43:07.249323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.shape: torch.Size([2, 1408])\n",
      "norms: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "image_paths = [\n",
    "    './img/choice.jpg',\n",
    "    './img/book.jpg'\n",
    "]\n",
    "base64_images = [image_to_base64(p) for p in image_paths]\n",
    "\n",
    "result = client(base64_images)\n",
    "embeddings = torch.Tensor(result['embeddings'])\n",
    "norms = torch.norm(embeddings, p=2, dim=1)\n",
    "\n",
    "print(f'embeddings.shape: {embeddings.shape}')\n",
    "print(f'norms: {norms}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d310d-1e0b-4ab1-abba-200b1101239d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T11:45:04.462190Z",
     "iopub.status.busy": "2025-04-04T11:45:04.461566Z",
     "iopub.status.idle": "2025-04-04T11:45:04.466658Z",
     "shell.execute_reply": "2025-04-04T11:45:04.465756Z",
     "shell.execute_reply.started": "2025-04-04T11:45:04.462174Z"
    }
   },
   "source": [
    "## 5. 开发 BLIP-2 客户端代码\n",
    "\n",
    "开发一个客户端，多线程地请求 BLIP-2 接口，将 CIFAR-100 数据集中的图片转成 Embedding，并保存到 data 目录下的 csv 文件。\n",
    "\n",
    "该开发流程在 [dec-pytorch](https://github.com/luochang212/dec-pytorch/blob/main/2.prepare_data.ipynb) 中亦有记载，就不详细展开了。这里将 `dec-pytorch` 的代码集成到 `utils.py` 直接调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b121f0-cd81-4311-890a-5d94de15db7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:20:32.290462Z",
     "iopub.status.busy": "2025-04-06T07:20:32.289932Z",
     "iopub.status.idle": "2025-04-06T09:53:03.142303Z",
     "shell.execute_reply": "2025-04-06T09:53:03.140317Z",
     "shell.execute_reply.started": "2025-04-06T07:20:32.290446Z"
    }
   },
   "outputs": [],
   "source": [
    "train_embeddings = utils.gen_image_embed(images=train_images[:SAMPLE_NUM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c61ce3c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T09:53:03.147932Z",
     "iopub.status.busy": "2025-04-06T09:53:03.147447Z",
     "iopub.status.idle": "2025-04-06T09:53:03.218010Z",
     "shell.execute_reply": "2025-04-06T09:53:03.217430Z",
     "shell.execute_reply.started": "2025-04-06T09:53:03.147884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embeddings</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.024523582309484482, -0.03633105754852295, 0...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.002521098591387272, 0.022899063304066658, ...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.008400454185903072, -0.012612388469278812, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.004734962247312069, -0.0035224033053964376...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.021240245550870895, -0.03918471559882164, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          embeddings  labels\n",
       "0  [0.024523582309484482, -0.03633105754852295, 0...      19\n",
       "1  [-0.002521098591387272, 0.022899063304066658, ...      29\n",
       "2  [0.008400454185903072, -0.012612388469278812, ...       0\n",
       "3  [-0.004734962247312069, -0.0035224033053964376...      11\n",
       "4  [-0.021240245550870895, -0.03918471559882164, ...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将 Embedding 和 label 存成 DataFrame\n",
    "embed_df = pd.DataFrame({\n",
    "    'embeddings': train_embeddings,\n",
    "    'labels': train_labels[:SAMPLE_NUM]\n",
    "})\n",
    "embed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65de76f0-c544-40a6-b3f6-8c53dbb00262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T09:53:03.219366Z",
     "iopub.status.busy": "2025-04-06T09:53:03.218870Z",
     "iopub.status.idle": "2025-04-06T09:53:17.568125Z",
     "shell.execute_reply": "2025-04-06T09:53:17.566802Z",
     "shell.execute_reply.started": "2025-04-06T09:53:03.219350Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将 DataFrame 存成 csv\n",
    "embed_csv_path = os.path.join(CSV_PATH, 'embed_label.csv')\n",
    "utils.embedding_df_to_csv(df=embed_df,\n",
    "                          csv_path=embed_csv_path,\n",
    "                          ebd_cols=['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd96d8cc-7bf9-43bb-9a00-29ad0e2afc7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T09:53:17.570790Z",
     "iopub.status.busy": "2025-04-06T09:53:17.570164Z",
     "iopub.status.idle": "2025-04-06T09:53:56.843541Z",
     "shell.execute_reply": "2025-04-06T09:53:56.842415Z",
     "shell.execute_reply.started": "2025-04-06T09:53:17.570755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将 csv 读入 DataFrame\n",
    "embed_df = utils.read_embedding_csv(csv_path=embed_csv_path,\n",
    "                                    ebd_cols=['embeddings'])\n",
    "len(embed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a437122-274e-4a4c-bb57-23d3b9767894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vllm_env)",
   "language": "python",
   "name": "vllm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
